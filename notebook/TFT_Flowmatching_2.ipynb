{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "cur_dir = os.path.dirname(os.path.abspath(\"__file__\"))  # Gets the current notebook directory\n",
    "src_dir = os.path.join(cur_dir, '../')  # Constructs the path to the 'src' directory\n",
    "# Add the 'src' directory to sys.path\n",
    "if src_dir not in sys.path:\n",
    "    sys.path.append(src_dir)\n",
    "\n",
    "from src.constant import *\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.MyDataset import MyDataset, save_dataset, load_dataset\n",
    "# from src.TFT_Flowmatching import TemporalFusionTransformerDiffusion\n",
    "\n",
    "from src.VQVAE import VQVAE\n",
    "from typing import Optional\n",
    "import pickle\n",
    "\n",
    "import torch.utils\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: direct load data from Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookback = 30\n",
    "future_steps = 40\n",
    "resample = False\n",
    "dir = '../data/Phase3/Modified/'\n",
    "ds = MyDataset(lookback=lookback)\n",
    "train_batch_size = 32\n",
    "test_batch_size = 256\n",
    "\n",
    "train = load_dataset('../data/.cache/train.pkl', batch_size=train_batch_size)\n",
    "test = load_dataset('../data/.cache/test.pkl', batch_size=test_batch_size)\n",
    "stats_dict = pickle.load(open('../data/.cache/stats_dict.pkl', 'rb'))\n",
    "\n",
    "# train = load_dataset('../data/.cache/opentraj_train.pkl', batch_size=train_batch_size)\n",
    "# test = load_dataset('../data/.cache/opentraj_test.pkl', batch_size=test_batch_size)\n",
    "# stats_dict = pickle.load(open('../data/.cache/opentraj_stats_dict.pkl', 'rb'))\n",
    "\n",
    "feature_dim = stats_dict['feature_dim']\n",
    "features = stats_dict['features']\n",
    "\n",
    "\n",
    "feature_dim = 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def process_data(df_dir : str, target_freq : int = 10):\n",
    "#     df: pd.DataFrame = pd.read_pickle(df_dir)\n",
    "#     df.columns = df.columns.str.strip() \n",
    "    \n",
    "#     df = df.rename(columns={'State': 'state'})\n",
    "\n",
    "#     states = ['At Station', 'Error', 'Wait', 'Cross', 'Approach Sidewalk',\n",
    "#        'Approach Target Station', 'Move Along Sidewalk']\n",
    "\n",
    "#     states_ohe = pd.get_dummies(df['state'], prefix='state')\n",
    "#     cur_states = df['state'].unique()\n",
    "#     for state in states:\n",
    "#         if state not in cur_states:\n",
    "#             states_ohe['state_'+state] = 0\n",
    "\n",
    "#     df = pd.concat([df, states_ohe], axis=1)\n",
    "#     df.drop(columns=['state'], inplace=True)\n",
    "    \n",
    "#     df.dropna(inplace=True, how='any')\n",
    "#     if resample:\n",
    "#         f_per_sec = df.groupby('TimestampID').count().mean().mean()\n",
    "#         if f_per_sec < target_freq:\n",
    "#             raise ValueError('The frequency of the data is lower than the target frequency')\n",
    "#         elif int(f_per_sec) == target_freq:\n",
    "#             pass\n",
    "#         else:\n",
    "#             resample_ratio = int(f_per_sec/target_freq)\n",
    "#             df = df.iloc[::resample_ratio, :]\n",
    "#     # # for origin\n",
    "#     for drop_column in ['Confidence', 'Timestamp', 'TimestampID', \n",
    "#                           'DatapointID', 'PID', 'SCN', 'U_X', 'U_Y', 'U_Z', \n",
    "#                           'AGV_Z', 'User_Z', 'GazeOrigin_Z', 'User_Pitch', 'User_Yaw', 'User_Roll', \n",
    "#                           'EyeTarget', \n",
    "#                           'start_station_X', 'start_station_Y', 'end_station_X', 'end_station_Y',\n",
    "#                           'distance_from_start_station_X',\n",
    "#                             'distance_from_start_station_Y', 'distance_from_end_station_X',\n",
    "#                             'distance_from_end_station_Y', 'facing_start_station',\n",
    "#                             'facing_end_station', \n",
    "#                             'rolling_avg', \n",
    "#                             'User', 'Type', \n",
    "#                             'possible_interaction'\n",
    "#                           ]:\n",
    "#         df = df.drop(columns=[drop_column], errors='ignore')\n",
    "\n",
    "#     target_columns = ['User_X', 'User_Y']\n",
    "#     # Reorder columns\n",
    "#     new_columns = target_columns + [col for col in df.columns if col not in target_columns]\n",
    "#     df = df[new_columns]\n",
    "    \n",
    "#     # keep numeric columns\n",
    "#     df = df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "#     return df\n",
    "\n",
    "# for file in os.listdir(dir):\n",
    "#     if file.endswith('.pkl'):\n",
    "#         df = process_data(dir+file)\n",
    "#         ds.read_data(df, agv_col_name=\"scenario\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = process_data(dir+file)\n",
    "# df = df[df['scenario'] == 7]\n",
    "\n",
    "# uer_x, uer_y = df['User_X'].values[10:40], df['User_Y'].values[10:40]\n",
    "\n",
    "# plt.plot(uer_x, uer_y)\n",
    "# # same\n",
    "# plt.title('User Position')\n",
    "# plt.xlabel('X')\n",
    "# plt.ylabel('Y')\n",
    "# # equal aspect ratio\n",
    "# plt.gca().set_aspect('equal', adjustable='box')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# random.shuffle(ds.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# stats_dict = {'mean': 0, 'std': 0, 'min': 0, 'max': 0}\n",
    "# stats_dict = ds.normalize_dataset()\n",
    "# ds.generate_data(future_steps=future_steps)\n",
    "\n",
    "# train:torch.utils.data.DataLoader\n",
    "# test:torch.utils.data.DataLoader\n",
    "\n",
    "# train, test = ds.split_data(frac=0.9, shuffle=True, train_batch_size=train_batch_size, test_batch_size=test_batch_size)\n",
    "\n",
    "# feature_dim = ds.feature_dim\n",
    "# stats_dict['feature_dim'] = feature_dim\n",
    "# stats_dict['features'] = ds.dataset[0].columns\n",
    "# columns = [_ for _ in ds.dataset[0].columns if _ not in ['AGV_name']]\n",
    "# print(f\"columns : {df.columns} \\nfeature_dim : {feature_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['User_X', 'User_Y', 'AGV_distance_X', 'AGV_distance_Y', 'AGV_speed_X',\n",
    "       'AGV_speed_Y', 'AGV_speed', 'User_speed_X', 'User_speed_Y',\n",
    "       'User_speed', 'User_velocity_X', 'User_velocity_Y', 'Wait_time',\n",
    "       'intent_to_cross', 'Gazing_station', 'possible_interaction',\n",
    "       'facing_along_sidewalk', 'facing_to_road', 'On_sidewalks', 'On_road',\n",
    "       'closest_station', 'distance_to_closest_station',\n",
    "       'distance_to_closest_station_X', 'distance_to_closest_station_Y',\n",
    "       'looking_at_AGV', 'GazeDirection_X', 'GazeDirection_Y',\n",
    "       'GazeDirection_Z', 'AGV_X', 'AGV_Y', 'looking_at_closest_station']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 30, 38]) torch.Size([32, 40, 38])\n",
      "317856 35328\n"
     ]
    }
   ],
   "source": [
    "for i, (X, y) in enumerate(train):\n",
    "    print(X.shape, y.shape)\n",
    "    break\n",
    "\n",
    "print(len(train) * train_batch_size, len(test) * test_batch_size)\n",
    "\n",
    "# # save it to cache to speed up\n",
    "# save_dataset(train, type='train', file_path='../data/.cache/train.pkl')\n",
    "# save_dataset(test, type='test', file_path='../data/.cache/test.pkl')\n",
    "# pickle.dump(stats_dict, open('../data/.cache/stats_dict.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of DiT parameters:  561024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.0245, 0.8891, 0.9995, 1.2931, 0.8837, 1.1130, 0.9513, 0.9853, 1.0225,\n",
       "        1.0871, 1.1273, 1.1970, 1.1731, 1.1876, 0.9925, 0.9115],\n",
       "       device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.DiT import DiT\n",
    "\n",
    "\n",
    "def compute_flow_target(noise, y_batch, t):\n",
    "    \"\"\"\n",
    "    Compute the intermediate sample x_t and target velocity for flow-matching.\n",
    "    \n",
    "    Args:\n",
    "        noise: Tensor of shape (batch, num_action_steps, action_dim), noise sample\n",
    "        y_batch: Tensor of shape (batch, num_action_steps, action_dim), ground truth actions\n",
    "        t: Tensor of shape (batch, 1, 1), time steps\n",
    "    \n",
    "    Returns:\n",
    "        x_t: Intermediate sample at time t\n",
    "        v_target: Target velocity\n",
    "    \"\"\"\n",
    "    t = t.view(-1, 1, 1)  # Ensure t is [batch, 1, 1]\n",
    "    x_t = t * noise + (1 - t) * y_batch\n",
    "\n",
    "    v_target = noise - y_batch\n",
    "    return x_t, v_target\n",
    "\n",
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes a sinusoidal embedding for a scalar timestep.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SinusoidalTimeEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t: Tensor of shape (batch,) or (batch, 1)\n",
    "        if len(t.shape) == 1:\n",
    "            t = t.unsqueeze(1)  # (batch, 1)\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        # Compute constant\n",
    "        emb_factor = math.log(10000) / (half_dim - 1)\n",
    "        # Create a tensor of shape (half_dim,)\n",
    "        dims = torch.arange(half_dim, device=t.device, dtype=t.dtype)\n",
    "        # (batch, half_dim)\n",
    "        emb = t * torch.exp(-dims * emb_factor)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        # If embedding_dim is odd, pad an extra zero.\n",
    "        if self.embedding_dim % 2 == 1:\n",
    "            emb = F.pad(emb, (0, 1))\n",
    "        return emb  # (batch, embedding_dim)\n",
    "\n",
    "class DiffusionDecoder(nn.Module):\n",
    "    def __init__(self, action_dim, conditioning_dim, num_diffusion_steps=10,\n",
    "                 num_action_steps=20, hidden_dim=128, num_layers=2, noise_weight=0.5, \n",
    "                 num_heads=4, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.num_diffusion_steps = num_diffusion_steps\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_action_steps = num_action_steps\n",
    "        self.noise_weight = noise_weight\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_embed = SinusoidalTimeEmbedding(hidden_dim)\n",
    "        self.time_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Input encoders\n",
    "        self.x_encoder = nn.Linear(action_dim, hidden_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_action_steps, hidden_dim))\n",
    "        self.conditioning_align = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        # Replace DiTBlock with DiT\n",
    "        self.dit = DiT(\n",
    "            num_attention_heads=num_heads,\n",
    "            attention_head_dim=hidden_dim // num_heads,\n",
    "            output_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_rate,\n",
    "            norm_elementwise_affine=True,\n",
    "            max_num_positional_embeddings=num_action_steps\n",
    "        )\n",
    "        \n",
    "        # Final projection layers with residual connections\n",
    "        self.final_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.proj_1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.proj_2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, conditioning, x_t, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            conditioning: Tensor of shape (batch, cond_len, hidden_dim)\n",
    "            x_t: Tensor of shape (batch, num_action_steps, action_dim)\n",
    "            t: Tensor of shape (batch,) with time values in [0,1]\n",
    "        \"\"\"\n",
    "        batch_size = x_t.size(0)\n",
    "        \n",
    "        # Encode input trajectory\n",
    "        x_encoded = self.x_encoder(x_t)\n",
    "        \n",
    "        # Combine encoded trajectory with time embedding\n",
    "        x = x_encoded + self.pos_embed\n",
    "        \n",
    "        # Process conditioning - average pooling as a simple approach\n",
    "        if conditioning.size(1) > 1:\n",
    "            cond_pooled = torch.mean(conditioning, dim=1, keepdim=True)  # [batch, 1, hidden_dim]\n",
    "        else:\n",
    "            cond_pooled = conditioning\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.time_embed(t)  # [batch, hidden_dim]\n",
    "        t_emb = self.time_proj(t_emb)  # Project time embedding\n",
    "        \n",
    "        # Broadcast time embedding to match sequence length\n",
    "        t_emb = t_emb.unsqueeze(1).repeat(1, x_t.size(1), 1)\n",
    "        \n",
    "        # Repeat conditioning for each timestep in the sequence\n",
    "        cond_expanded = cond_pooled.repeat(1, x_t.size(1), 1)\n",
    "        cond_expanded = torch.cat([cond_expanded, t_emb], dim=-1)\n",
    "        cond_expanded = self.conditioning_align(cond_expanded)\n",
    "\n",
    "        # Process through DiT model (instead of iterating through DiTBlocks)\n",
    "        # Convert timestep to the format DiT expects\n",
    "        timesteps = (t * 1000).long()  # Scale to typical timestep range\n",
    "        \n",
    "        # Pass through the DiT model\n",
    "        x = self.dit(\n",
    "            hidden_states=x,\n",
    "            encoder_hidden_states=cond_expanded,\n",
    "            timestep=timesteps\n",
    "        )\n",
    "        \n",
    "        # Final processing\n",
    "        x = self.final_norm(x)\n",
    "        x = F.gelu(self.proj_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.proj_2(x))\n",
    "        x = self.dropout(x)\n",
    "        output = self.output(x)\n",
    "        \n",
    "        return output  # [batch, num_action_steps, action_dim]\n",
    "\n",
    "    def decoder_train_step(self, conditioning, y_batch, device):\n",
    "        \"\"\"\n",
    "        Performs one training step for the flow-matching decoder.\n",
    "        \n",
    "        Args:\n",
    "            conditioning: Tensor of shape (batch, cond_len, conditioning_dim)\n",
    "            y_batch: Ground truth trajectory (batch, num_action_steps, action_dim)\n",
    "            device: torch.device\n",
    "        \n",
    "        Returns:\n",
    "            loss: The MSE loss between predicted and target velocity\n",
    "        \"\"\"\n",
    "        batch_size = y_batch.size(0)\n",
    "        # Sample t uniformly from [0,1]\n",
    "        t = torch.rand(batch_size, device=device)\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(y_batch) * self.noise_weight\n",
    "        \n",
    "        # Compute x_t and v_target\n",
    "        x_t, v_target = compute_flow_target(noise, y_batch, t.unsqueeze(1).unsqueeze(2))\n",
    "        \n",
    "        # Predict velocity\n",
    "        v_pred = self.forward(conditioning, x_t, t)\n",
    "        \n",
    "        # MSE loss\n",
    "        loss = F.mse_loss(v_pred, v_target, reduction='none')\n",
    "        loss = loss.mean(dim=[1, 2])  # Average over action steps and dimensions\n",
    "        return loss\n",
    "    \n",
    "    def influence(self, conditioning, device):\n",
    "        \"\"\"\n",
    "        Runs the flow-matching integration process and returns a list of intermediate trajectories.\n",
    "        \n",
    "        Args:\n",
    "            conditioning: Tensor of shape (batch, cond_len, conditioning_dim)\n",
    "            device: torch.device\n",
    "        \n",
    "        Returns:\n",
    "            intermediates: A list of tensors, each of shape (batch, num_action_steps, action_dim)\n",
    "        \"\"\"\n",
    "        batch_size = conditioning.size(0)\n",
    "        x = torch.randn(batch_size, self.num_action_steps, self.action_dim, device=device) * self.noise_weight\n",
    "        intermediates = []\n",
    "        \n",
    "        # Integration step size (negative for backward integration)\n",
    "        dt = -1.0 / self.num_diffusion_steps\n",
    "        \n",
    "        # Run the diffusion process\n",
    "        for i in range(self.num_diffusion_steps):\n",
    "            t = 1.0 + i * dt  # t decreases from 1.0 to almost 0\n",
    "            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.float)\n",
    "            v_pred = self.forward(conditioning, x, t_tensor)\n",
    "            x = x + v_pred * dt  # Since dt < 0, moves x towards data\n",
    "            intermediates.append(x.clone())\n",
    "            \n",
    "        return intermediates\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "decoder = DiffusionDecoder(action_dim=2, conditioning_dim=feature_dim, num_diffusion_steps=10, num_action_steps=future_steps, hidden_dim=128, num_layers=2, noise_weight=0.1, num_heads=4)\n",
    "y_batch = torch.randn(16, 40, 2).to(device)\n",
    "condition = torch.randn(16, 1, 128).to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "decoder.decoder_train_step(condition, y_batch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of DiT parameters:  561024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1.1384, 0.9550, 1.1057, 0.9405, 0.9709, 0.8395, 1.1650, 0.9357, 1.0931,\n",
       "        1.1338, 1.1170, 0.7089, 0.8132, 0.9298, 1.0711, 0.8744],\n",
       "       device='cuda:0', grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.DiT import DiT\n",
    "\n",
    "\n",
    "def compute_flow_target(noise, y_batch, t):\n",
    "    \"\"\"\n",
    "    Compute the intermediate sample x_t and target velocity for flow-matching.\n",
    "    \n",
    "    Args:\n",
    "        noise: Tensor of shape (batch, num_action_steps, action_dim), noise sample\n",
    "        y_batch: Tensor of shape (batch, num_action_steps, action_dim), ground truth actions\n",
    "        t: Tensor of shape (batch, 1, 1), time steps\n",
    "    \n",
    "    Returns:\n",
    "        x_t: Intermediate sample at time t\n",
    "        v_target: Target velocity\n",
    "    \"\"\"\n",
    "    t = t.view(-1, 1, 1)  # Ensure t is [batch, 1, 1]\n",
    "    x_t = t * noise + (1 - t) * y_batch\n",
    "\n",
    "    v_target = noise - y_batch\n",
    "    return x_t, v_target\n",
    "\n",
    "class SinusoidalTimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes a sinusoidal embedding for a scalar timestep.\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(SinusoidalTimeEmbedding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def forward(self, t):\n",
    "        # t: Tensor of shape (batch,) or (batch, 1)\n",
    "        if len(t.shape) == 1:\n",
    "            t = t.unsqueeze(1)  # (batch, 1)\n",
    "        half_dim = self.embedding_dim // 2\n",
    "        # Compute constant\n",
    "        emb_factor = math.log(10000) / (half_dim - 1)\n",
    "        # Create a tensor of shape (half_dim,)\n",
    "        dims = torch.arange(half_dim, device=t.device, dtype=t.dtype)\n",
    "        # (batch, half_dim)\n",
    "        emb = t * torch.exp(-dims * emb_factor)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        # If embedding_dim is odd, pad an extra zero.\n",
    "        if self.embedding_dim % 2 == 1:\n",
    "            emb = F.pad(emb, (0, 1))\n",
    "        return emb  # (batch, embedding_dim)\n",
    "\n",
    "class DiffusionDecoder(nn.Module):\n",
    "    def __init__(self, action_dim, conditioning_dim, num_diffusion_steps=10,\n",
    "                 num_action_steps=20, hidden_dim=128, num_layers=2, noise_weight=0.5, \n",
    "                 num_heads=4, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.num_diffusion_steps = num_diffusion_steps\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_action_steps = num_action_steps\n",
    "        self.noise_weight = noise_weight\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Time embedding\n",
    "        self.time_embed = SinusoidalTimeEmbedding(hidden_dim)\n",
    "        self.time_proj = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        )\n",
    "        \n",
    "        # Input encoders\n",
    "        self.x_encoder = nn.Linear(action_dim, hidden_dim)\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_action_steps, hidden_dim))\n",
    "        self.conditioning_align = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        # Replace DiTBlock with DiT\n",
    "        self.dit = DiT(\n",
    "            num_attention_heads=num_heads,\n",
    "            attention_head_dim=hidden_dim // num_heads,\n",
    "            output_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout_rate,\n",
    "            norm_elementwise_affine=True,\n",
    "            max_num_positional_embeddings=num_action_steps\n",
    "        )\n",
    "        \n",
    "        # Final projection layers with residual connections\n",
    "        self.final_norm = nn.LayerNorm(hidden_dim)\n",
    "        self.proj_1 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.proj_2 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, conditioning, x_t, t):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            conditioning: Tensor of shape (batch, cond_len, hidden_dim)\n",
    "            x_t: Tensor of shape (batch, num_action_steps, action_dim)\n",
    "            t: Tensor of shape (batch,) with time values in [0,1]\n",
    "        \"\"\"\n",
    "        batch_size = x_t.size(0)\n",
    "        \n",
    "        # Encode input trajectory\n",
    "        x_encoded = self.x_encoder(x_t)\n",
    "        \n",
    "        # Combine encoded trajectory with time embedding\n",
    "        x = x_encoded + self.pos_embed\n",
    "        \n",
    "        # Process conditioning - average pooling as a simple approach\n",
    "        if conditioning.size(1) > 1:\n",
    "            cond_pooled = torch.mean(conditioning, dim=1, keepdim=True)  # [batch, 1, hidden_dim]\n",
    "        else:\n",
    "            cond_pooled = conditioning\n",
    "        \n",
    "        # Time embedding\n",
    "        t_emb = self.time_embed(t)  # [batch, hidden_dim]\n",
    "        t_emb = self.time_proj(t_emb)  # Project time embedding\n",
    "        \n",
    "        # Broadcast time embedding to match sequence length\n",
    "        t_emb = t_emb.unsqueeze(1).repeat(1, x_t.size(1), 1)\n",
    "        \n",
    "        # Repeat conditioning for each timestep in the sequence\n",
    "        cond_expanded = cond_pooled.repeat(1, x_t.size(1), 1)\n",
    "        cond_expanded = torch.cat([cond_expanded, t_emb], dim=-1)\n",
    "        cond_expanded = self.conditioning_align(cond_expanded)\n",
    "\n",
    "        # Process through DiT model (instead of iterating through DiTBlocks)\n",
    "        # Convert timestep to the format DiT expects\n",
    "        timesteps = (t * 1000).long()  # Scale to typical timestep range\n",
    "        \n",
    "        # Pass through the DiT model\n",
    "        x = self.dit(\n",
    "            hidden_states=x,\n",
    "            encoder_hidden_states=cond_expanded,\n",
    "            timestep=timesteps\n",
    "        )\n",
    "        \n",
    "        # Final processing\n",
    "        x = self.final_norm(x)\n",
    "        x = F.gelu(self.proj_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.gelu(self.proj_2(x))\n",
    "        x = self.dropout(x)\n",
    "        output = self.output(x)\n",
    "        \n",
    "        return output  # [batch, num_action_steps, action_dim]\n",
    "\n",
    "    def decoder_train_step(self, conditioning, y_batch, device):\n",
    "        \"\"\"\n",
    "        Performs one training step for the flow-matching decoder.\n",
    "        \n",
    "        Args:\n",
    "            conditioning: Tensor of shape (batch, cond_len, conditioning_dim)\n",
    "            y_batch: Ground truth trajectory (batch, num_action_steps, action_dim)\n",
    "            device: torch.device\n",
    "        \n",
    "        Returns:\n",
    "            loss: The MSE loss between predicted and target velocity\n",
    "        \"\"\"\n",
    "        batch_size = y_batch.size(0)\n",
    "        # Sample t uniformly from [0,1]\n",
    "        t = torch.rand(batch_size, device=device)\n",
    "        \n",
    "        # Sample noise\n",
    "        noise = torch.randn_like(y_batch) * self.noise_weight\n",
    "        \n",
    "        # Compute x_t and v_target\n",
    "        x_t, v_target = compute_flow_target(noise, y_batch, t.unsqueeze(1).unsqueeze(2))\n",
    "        \n",
    "        # Predict velocity\n",
    "        v_pred = self.forward(conditioning, x_t, t)\n",
    "        \n",
    "        # MSE loss\n",
    "        loss = F.mse_loss(v_pred, v_target, reduction='none')\n",
    "        loss = loss.mean(dim=[1, 2])  # Average over action steps and dimensions\n",
    "        return loss\n",
    "    \n",
    "    def influence(self, conditioning, device):\n",
    "        \"\"\"\n",
    "        Runs the flow-matching integration process and returns a list of intermediate trajectories.\n",
    "        \n",
    "        Args:\n",
    "            conditioning: Tensor of shape (batch, cond_len, conditioning_dim)\n",
    "            device: torch.device\n",
    "        \n",
    "        Returns:\n",
    "            intermediates: A list of tensors, each of shape (batch, num_action_steps, action_dim)\n",
    "        \"\"\"\n",
    "        batch_size = conditioning.size(0)\n",
    "        x = torch.randn(batch_size, self.num_action_steps, self.action_dim, device=device) * self.noise_weight\n",
    "        intermediates = []\n",
    "        \n",
    "        # Integration step size (negative for backward integration)\n",
    "        dt = -1.0 / self.num_diffusion_steps\n",
    "        \n",
    "        # Run the diffusion process\n",
    "        for i in range(self.num_diffusion_steps):\n",
    "            t = 1.0 + i * dt  # t decreases from 1.0 to almost 0\n",
    "            t_tensor = torch.full((batch_size,), t, device=device, dtype=torch.float)\n",
    "            v_pred = self.forward(conditioning, x, t_tensor)\n",
    "            x = x + v_pred * dt  # Since dt < 0, moves x towards data\n",
    "            intermediates.append(x.clone())\n",
    "            \n",
    "        return intermediates\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "decoder = DiffusionDecoder(action_dim=2, conditioning_dim=feature_dim, num_diffusion_steps=10, num_action_steps=future_steps, hidden_dim=128, num_layers=2, noise_weight=0.1, num_heads=4)\n",
    "y_batch = torch.randn(16, 40, 2).to(device)\n",
    "condition = torch.randn(16, 1, 128).to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "decoder.decoder_train_step(condition, y_batch, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "from src.VQVAE import VQVAE\n",
    "from src.DiT import SelfAttentionTransformer\n",
    "import math\n",
    "\n",
    "###############################################\n",
    "# Original Blocks (with minor efficiency tweaks)\n",
    "###############################################\n",
    "\n",
    "class GatedResidualNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=3, dropout_rate=0.1):\n",
    "        super(GatedResidualNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(input_size if i == 0 else hidden_size, hidden_size) for i in range(num_layers)]\n",
    "        )\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(hidden_size) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.fc3 = nn.Linear(input_size, output_size)\n",
    "        self.gate = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_input = x\n",
    "        for layer, norm in zip(self.layers, self.norms):\n",
    "            x = F.relu(layer(x))\n",
    "            x = self.dropout(x)\n",
    "            x = norm(x)\n",
    "        gate = torch.sigmoid(self.gate(x))\n",
    "        x2 = self.fc2(x)\n",
    "        return self.fc3(x_input) + gate * x2\n",
    "    \n",
    "    \n",
    "class GRUEncoder(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super(GRUEncoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # update gate parameters\n",
    "        self.x2z = nn.Linear(input_dim,  hidden_dim)\n",
    "        self.h2z = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        # reset  gate parameters\n",
    "        self.x2r = nn.Linear(input_dim,  hidden_dim)\n",
    "        self.h2r = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        # candidate hidden state parameters\n",
    "        self.x2h = nn.Linear(input_dim,  hidden_dim)\n",
    "        self.h2h = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, h0: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x:  Tensor of shape (batch, seq_len, input_dim)\n",
    "          h0: Optional initial hidden state, shape (batch, hidden_dim)\n",
    "        Returns:\n",
    "          outputs: Tensor of shape (batch, seq_len, hidden_dim)\n",
    "          h_t:      Tensor of shape (batch, hidden_dim), the final hidden state\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        # initialize hidden state if not provided\n",
    "        if h0 is None:\n",
    "            h_t = x.new_zeros(batch_size, self.hidden_dim)\n",
    "        else:\n",
    "            h_t = h0\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            # compute gates\n",
    "            z_t = torch.sigmoid(self.x2z(x_t) + self.h2z(h_t))      # update gate\n",
    "            r_t = torch.sigmoid(self.x2r(x_t) + self.h2r(h_t))      # reset  gate\n",
    "            \n",
    "            # candidate hidden state\n",
    "            h_tilde = torch.tanh(self.x2h(x_t) + self.h2h(r_t * h_t))\n",
    "            \n",
    "            # new hidden state\n",
    "            h_t = (1 - z_t) * h_t + z_t * h_tilde\n",
    "\n",
    "            outputs.append(h_t.unsqueeze(1))\n",
    "\n",
    "        outputs = torch.cat(outputs, dim=1)  # (batch, seq_len, hidden_dim)\n",
    "        return outputs, h_t\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads, dropout_rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(hidden_size, num_heads, dropout=dropout_rate)\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.norm2 = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size * 4, hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask: Optional[torch.Tensor] = None):\n",
    "        # x: (seq_len, batch, hidden_size)\n",
    "        x2 = x\n",
    "        x = self.norm1(x)\n",
    "        x, _ = self.attention(x, x, x, key_padding_mask=mask)\n",
    "        x = x + x2\n",
    "        x2 = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.feed_forward(x)\n",
    "        x = x + x2\n",
    "        return x\n",
    "\n",
    "\n",
    "def cosine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "\n",
    "class TemporalFusionTransformerDiffusion(nn.Module):\n",
    "    def __init__(self, num_features, num_hidden, num_outputs, num_steps, his_steps = 30, \n",
    "                 num_attention_heads=8, diffusion_steps=10, vqvae: VQVAE = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_features (int): Number of input features.\n",
    "            num_hidden (int): Hidden dimension size.\n",
    "            num_outputs (int): Dimensionality of each output (e.g. action dimension).\n",
    "            num_steps (int): Desired output sequence length (e.g. number of action steps).\n",
    "            num_attention_heads (int): Number of heads for the transformer blocks.\n",
    "            diffusion_steps (int): Number of diffusion (denoising) steps.\n",
    "        \"\"\"\n",
    "        super(TemporalFusionTransformerDiffusion, self).__init__()\n",
    "        if vqvae is None:\n",
    "            self.vqvae = VQVAE(input_dim=feature_dim, hidden_dim=512, num_embeddings=128, embedding_dim=128, commitment_cost=0.25)\n",
    "        else:\n",
    "            self.vqvae = vqvae\n",
    "\n",
    "        self.num_hidden = num_hidden\n",
    "        num_features = num_features + self.vqvae.encoder.fc2.out_features\n",
    "        self.encoder_grn = GatedResidualNetwork(num_features, num_hidden, num_hidden)\n",
    "        \n",
    "        # self.transformer_block = TransformerBlock(num_hidden, num_heads=num_attention_heads, dropout_rate=0.1)\n",
    "        # self.transformer_block2 = TransformerBlock(num_hidden, num_heads=num_attention_heads, dropout_rate=0.1)\n",
    "        \n",
    "        self.transformer_encoder = SelfAttentionTransformer(num_attention_heads=num_attention_heads, \n",
    "                                                            attention_head_dim=num_hidden // num_attention_heads, \n",
    "                                                            output_dim=num_hidden, \n",
    "                                                            num_layers=2, \n",
    "                                                            dropout=0.1, attention_bias=True, \n",
    "                                                            activation_fn=\"gelu-approximate\", \n",
    "                                                            max_num_positional_embeddings=512, \n",
    "                                                            compute_dtype=torch.float32, \n",
    "                                                            final_dropout=True, \n",
    "                                                            positional_embeddings=\"sinusoidal\", \n",
    "                                                            interleave_self_attention=False)\n",
    "        self.encoder_gru = GRUEncoder(num_hidden, num_hidden)\n",
    "        self.his_steps = his_steps\n",
    "\n",
    "        # Diffusion decoder: we set action_dim=num_outputs and produce a sequence of length num_steps.\n",
    "        self.diffusion_decoder = DiffusionDecoder(\n",
    "            action_dim=num_outputs,\n",
    "            conditioning_dim=num_hidden,\n",
    "            num_diffusion_steps=diffusion_steps,\n",
    "            num_action_steps=num_steps,\n",
    "            num_heads=num_attention_heads,  \n",
    "            hidden_dim=num_hidden, \n",
    "            num_layers=2,  # you can adjust as needed\n",
    "            noise_weight=0.5  # you can adjust as needed\n",
    "        )\n",
    "\n",
    "        self.num_steps = num_steps\n",
    "        self.num_outputs = num_outputs\n",
    "\n",
    "\n",
    "    def forward(self, x, y_batch=None , mask: Optional[torch.Tensor] = None, influence=False, return_all=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, seq_len, num_features).\n",
    "            mask: Optional attention mask for the transformer blocks.\n",
    "            \n",
    "        Returns:\n",
    "            actions: Tensor of shape (batch, num_steps, num_outputs)\n",
    "        \"\"\"\n",
    "        # If given a 2D input, add a batch dimension.\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "\n",
    "        # VQ-VAE\n",
    "        x_recon, vq_loss, perplexity, embedding = self.vqvae(x)\n",
    "        x = torch.cat((x, embedding), dim=-1)\n",
    "        \n",
    "        # Encoder GRN.\n",
    "        x = self.encoder_grn(x)  # (batch, seq_len, num_hidden)\n",
    "        \n",
    "        # Transformer expects (seq_len, batch, hidden_size).\n",
    "        # x = x.permute(1, 0, 2)\n",
    "        # x = self.transformer_block(x, mask=mask)\n",
    "        # x = self.transformer_block2(x, mask=mask)\n",
    "        # x = x.permute(1, 0, 2)  # back to (batch, seq_len, num_hidden)\n",
    "        \n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "\n",
    "        # Use a summary of the encoder output as conditioning.\n",
    "        # Here we use the last time–step (you might also try an average or more complex pooling).\n",
    "        x, conditioning = self.encoder_gru(x)  # (batch, seq_len, num_hidden)\n",
    "        conditioning = conditioning.unsqueeze(1)\n",
    "        # conditioning = x[:, -1:, :] #self.condition_proj()  # (batch, 1, num_hidden)\n",
    "\n",
    "        # flow matching during training\n",
    "        self.device = next(self.parameters()).device\n",
    "        \n",
    "        if influence:\n",
    "            if return_all:\n",
    "                return [traj for traj in self.diffusion_decoder.influence(conditioning, self.device)]\n",
    "            return self.diffusion_decoder.influence(conditioning, self.device)[-1]\n",
    "        else:\n",
    "            if self.training:\n",
    "                max_displace = torch.max(y_batch, dim=1).values - torch.min(y_batch, dim=1).values\n",
    "                max_displace = torch.linalg.norm(max_displace, dim=1)\n",
    "                diff_loss = self.diffusion_decoder.decoder_train_step(conditioning, y_batch, self.device)\n",
    "                diff_loss = diff_loss.mean()\n",
    "                return diff_loss, vq_loss\n",
    "            \n",
    "\n",
    "    def influence(self, x):\n",
    "        User_trajectory = self.forward(x, influence=True)\n",
    "        return User_trajectory\n",
    "\n",
    "\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"Using {device}\")\n",
    "\n",
    "# vqvae = VQVAE(input_dim=feature_dim, hidden_dim=512, num_embeddings=128, embedding_dim=128, commitment_cost=0.25)\n",
    "\n",
    "# model = TemporalFusionTransformerDiffusion(num_features=feature_dim, num_hidden=128, num_outputs=2, num_steps=future_steps, diffusion_steps=10, vqvae=vqvae)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# model.to(device)\n",
    "\n",
    "# X_batch, y_batch = next(iter(train))\n",
    "# X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "# model(X_batch, y_batch[:, :future_steps, :2], influence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Total number of SelfAttentionTransformer parameters:  396544\n",
      "Total number of DiT parameters:  561024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecayLoss(nn.Module):\n",
    "    def __init__(self, num_steps, baseline_loss_fn=nn.L1Loss()):\n",
    "        super(DecayLoss, self).__init__()\n",
    "        # Weight decreases as we move further into the future\n",
    "        self.weights = torch.linspace(1.0, 1.0, num_steps)\n",
    "        self.baseline_loss_fn = baseline_loss_fn\n",
    "        \n",
    "\n",
    "    def forward(self, predictions, targets):\n",
    "        loss = 0\n",
    "        for i in range(predictions.shape[1]):\n",
    "            loss += self.weights[i] * self.baseline_loss_fn(predictions[:, i], targets[:, i])\n",
    "        return loss.mean()\n",
    "    \n",
    "    \n",
    "baseline_loss_fn = nn.L1Loss() #nn.MSELoss()\n",
    "loss_fn = DecayLoss(future_steps, baseline_loss_fn=baseline_loss_fn)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "vqvae = VQVAE(input_dim=feature_dim, hidden_dim=512, num_embeddings=128, embedding_dim=128, commitment_cost=0.25)\n",
    "\n",
    "model = TemporalFusionTransformerDiffusion(num_features=feature_dim, num_hidden=128, num_outputs=2, num_steps=future_steps, diffusion_steps=5, vqvae=vqvae)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "model.to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(\"/home/shaoze/Documents/Prediction/Pedestrian-Trajectory-Prediction/model/TFT_Flowmatching/SFT/model_10000.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer with early stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model at ../model/TFT_Flowmatching/Jun02_00-34-31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9231b8f32bf84703abda258b6a6bde12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9933 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 500, Loss: 0.048216, VQ Loss: 0.011438, Diff Loss: 0.036778, learning rate: 5.00e-05\n",
      "Epoch 1, Step 1000, Loss: 0.043761, VQ Loss: 0.009100, Diff Loss: 0.034661, learning rate: 5.00e-05\n",
      "Epoch 1, Step 1500, Loss: 0.043262, VQ Loss: 0.007745, Diff Loss: 0.035516, learning rate: 5.00e-05\n",
      "Epoch 1, Step 2000, Loss: 0.042232, VQ Loss: 0.006851, Diff Loss: 0.035381, learning rate: 5.00e-05\n",
      "Epoch 1, Step 2500, Loss: 0.040283, VQ Loss: 0.006176, Diff Loss: 0.034107, learning rate: 5.00e-05\n",
      "Epoch 1, Step 3000, Loss: 0.037229, VQ Loss: 0.005533, Diff Loss: 0.031696, learning rate: 5.00e-05\n",
      "Epoch 1, Step 3500, Loss: 0.037073, VQ Loss: 0.005091, Diff Loss: 0.031982, learning rate: 5.00e-05\n",
      "Epoch 1, Step 4000, Loss: 0.036673, VQ Loss: 0.004769, Diff Loss: 0.031905, learning rate: 5.00e-05\n",
      "Epoch 1, Step 4500, Loss: 0.036418, VQ Loss: 0.004544, Diff Loss: 0.031874, learning rate: 5.00e-05\n",
      "Epoch 1, Step 5000, Loss: 0.036817, VQ Loss: 0.004398, Diff Loss: 0.032419, learning rate: 5.00e-05\n",
      "Steps 5000: test RMSE 1.1527, moving average RMSE 1.1527, learning rate 5.00e-05\n",
      "Epoch 1, Step 5500, Loss: 0.034433, VQ Loss: 0.004127, Diff Loss: 0.030306, learning rate: 5.00e-05\n",
      "Epoch 1, Step 6000, Loss: 0.032771, VQ Loss: 0.003902, Diff Loss: 0.028869, learning rate: 1.00e-05\n",
      "Epoch 1, Step 6500, Loss: 0.031787, VQ Loss: 0.003776, Diff Loss: 0.028011, learning rate: 1.00e-05\n",
      "Epoch 1, Step 7000, Loss: 0.031675, VQ Loss: 0.003712, Diff Loss: 0.027963, learning rate: 1.00e-05\n",
      "Epoch 1, Step 7500, Loss: 0.033951, VQ Loss: 0.003712, Diff Loss: 0.030239, learning rate: 1.00e-05\n",
      "Epoch 1, Step 8000, Loss: 0.033289, VQ Loss: 0.003685, Diff Loss: 0.029604, learning rate: 1.00e-05\n",
      "Epoch 1, Step 8500, Loss: 0.032286, VQ Loss: 0.003635, Diff Loss: 0.028650, learning rate: 1.00e-05\n",
      "Epoch 1, Step 9000, Loss: 0.033830, VQ Loss: 0.003625, Diff Loss: 0.030205, learning rate: 1.00e-05\n",
      "Epoch 1, Step 9500, Loss: 0.031312, VQ Loss: 0.003475, Diff Loss: 0.027836, learning rate: 2.00e-06\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f6d512d65049eb81fbd307a69c7a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9933 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Step 67, Loss: 0.033430, VQ Loss: 0.003507, Diff Loss: 0.029923, learning rate: 2.00e-06\n",
      "Model saved at ../model/TFT_Flowmatching/Jun02_00-34-31/model_10000.pt\n",
      "Steps 10000: test RMSE 1.0494, moving average RMSE 1.0907, learning rate 2.00e-06\n",
      "Epoch 2, Step 567, Loss: 0.031269, VQ Loss: 0.003455, Diff Loss: 0.027814, learning rate: 2.00e-06\n",
      "Epoch 2, Step 1067, Loss: 0.031655, VQ Loss: 0.003503, Diff Loss: 0.028152, learning rate: 2.00e-06\n",
      "Epoch 2, Step 1567, Loss: 0.029909, VQ Loss: 0.003495, Diff Loss: 0.026414, learning rate: 2.00e-06\n",
      "Epoch 2, Step 2067, Loss: 0.032598, VQ Loss: 0.003470, Diff Loss: 0.029129, learning rate: 2.00e-06\n",
      "Epoch 2, Step 2567, Loss: 0.031388, VQ Loss: 0.003465, Diff Loss: 0.027923, learning rate: 2.00e-06\n",
      "Epoch 2, Step 3067, Loss: 0.032806, VQ Loss: 0.003512, Diff Loss: 0.029294, learning rate: 4.00e-07\n",
      "Epoch 2, Step 3567, Loss: 0.030389, VQ Loss: 0.003531, Diff Loss: 0.026858, learning rate: 4.00e-07\n",
      "Epoch 2, Step 4067, Loss: 0.030750, VQ Loss: 0.003423, Diff Loss: 0.027327, learning rate: 4.00e-07\n",
      "Epoch 2, Step 4567, Loss: 0.029749, VQ Loss: 0.003371, Diff Loss: 0.026379, learning rate: 4.00e-07\n",
      "Epoch 2, Step 5067, Loss: 0.030956, VQ Loss: 0.003483, Diff Loss: 0.027473, learning rate: 4.00e-07\n",
      "Steps 15000: test RMSE 1.0340, moving average RMSE 1.0567, learning rate 4.00e-07\n",
      "Epoch 2, Step 5567, Loss: 0.029649, VQ Loss: 0.003445, Diff Loss: 0.026204, learning rate: 4.00e-07\n",
      "Epoch 2, Step 6067, Loss: 0.031208, VQ Loss: 0.003448, Diff Loss: 0.027760, learning rate: 8.00e-08\n",
      "Epoch 2, Step 6567, Loss: 0.031384, VQ Loss: 0.003410, Diff Loss: 0.027975, learning rate: 8.00e-08\n",
      "Epoch 2, Step 7067, Loss: 0.030871, VQ Loss: 0.003427, Diff Loss: 0.027444, learning rate: 8.00e-08\n",
      "Epoch 2, Step 7567, Loss: 0.030879, VQ Loss: 0.003444, Diff Loss: 0.027435, learning rate: 8.00e-08\n",
      "Epoch 2, Step 8067, Loss: 0.030565, VQ Loss: 0.003428, Diff Loss: 0.027137, learning rate: 8.00e-08\n",
      "Epoch 2, Step 8567, Loss: 0.030248, VQ Loss: 0.003433, Diff Loss: 0.026816, learning rate: 8.00e-08\n",
      "Epoch 2, Step 9067, Loss: 0.031241, VQ Loss: 0.003418, Diff Loss: 0.027823, learning rate: 1.60e-08\n",
      "Epoch 2, Step 9567, Loss: 0.031786, VQ Loss: 0.003473, Diff Loss: 0.028313, learning rate: 1.60e-08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a0aac8ca024ef38c2b727fc5c40c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9933 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 134, Loss: 0.032159, VQ Loss: 0.003478, Diff Loss: 0.028681, learning rate: 1.60e-08\n",
      "Model saved at ../model/TFT_Flowmatching/Jun02_00-34-31/model_20000.pt\n",
      "Steps 20000: test RMSE 1.0353, moving average RMSE 1.0438, learning rate 1.60e-08\n",
      "Epoch 3, Step 634, Loss: 0.032684, VQ Loss: 0.003502, Diff Loss: 0.029182, learning rate: 1.60e-08\n",
      "Epoch 3, Step 1134, Loss: 0.033250, VQ Loss: 0.003379, Diff Loss: 0.029872, learning rate: 1.60e-08\n",
      "Epoch 3, Step 1634, Loss: 0.032114, VQ Loss: 0.003467, Diff Loss: 0.028647, learning rate: 1.60e-08\n",
      "Epoch 3, Step 2134, Loss: 0.031773, VQ Loss: 0.003479, Diff Loss: 0.028294, learning rate: 1.60e-08\n",
      "Epoch 3, Step 2634, Loss: 0.030178, VQ Loss: 0.003396, Diff Loss: 0.026782, learning rate: 1.60e-08\n",
      "Epoch 3, Step 3134, Loss: 0.031409, VQ Loss: 0.003416, Diff Loss: 0.027993, learning rate: 1.60e-08\n",
      "Epoch 3, Step 3634, Loss: 0.032329, VQ Loss: 0.003466, Diff Loss: 0.028863, learning rate: 1.60e-08\n",
      "Epoch 3, Step 4134, Loss: 0.029789, VQ Loss: 0.003369, Diff Loss: 0.026420, learning rate: 1.60e-08\n",
      "Epoch 3, Step 4634, Loss: 0.033371, VQ Loss: 0.003446, Diff Loss: 0.029925, learning rate: 1.60e-08\n",
      "Epoch 3, Step 5134, Loss: 0.030462, VQ Loss: 0.003449, Diff Loss: 0.027013, learning rate: 1.60e-08\n",
      "Steps 25000: test RMSE 1.0365, moving average RMSE 1.0394, learning rate 1.60e-08\n",
      "Epoch 3, Step 5634, Loss: 0.030091, VQ Loss: 0.003436, Diff Loss: 0.026655, learning rate: 1.60e-08\n",
      "Epoch 3, Step 6134, Loss: 0.031264, VQ Loss: 0.003418, Diff Loss: 0.027846, learning rate: 1.60e-08\n",
      "Epoch 3, Step 6634, Loss: 0.030458, VQ Loss: 0.003447, Diff Loss: 0.027011, learning rate: 1.60e-08\n",
      "Epoch 3, Step 7134, Loss: 0.032164, VQ Loss: 0.003506, Diff Loss: 0.028659, learning rate: 1.60e-08\n",
      "Epoch 3, Step 7634, Loss: 0.032259, VQ Loss: 0.003397, Diff Loss: 0.028862, learning rate: 1.60e-08\n",
      "Epoch 3, Step 8134, Loss: 0.031425, VQ Loss: 0.003415, Diff Loss: 0.028009, learning rate: 1.60e-08\n",
      "Epoch 3, Step 8634, Loss: 0.033248, VQ Loss: 0.003471, Diff Loss: 0.029778, learning rate: 1.60e-08\n",
      "Epoch 3, Step 9134, Loss: 0.032749, VQ Loss: 0.003518, Diff Loss: 0.029231, learning rate: 1.60e-08\n",
      "Epoch 3, Step 9634, Loss: 0.030943, VQ Loss: 0.003414, Diff Loss: 0.027528, learning rate: 1.60e-08\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "920b5b1eeb3b4387b39e2e676dbfaba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9933 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Step 201, Loss: 0.032883, VQ Loss: 0.003493, Diff Loss: 0.029390, learning rate: 1.60e-08\n",
      "Model saved at ../model/TFT_Flowmatching/Jun02_00-34-31/model_30000.pt\n",
      "Steps 30000: test RMSE 1.0363, moving average RMSE 1.0375, learning rate 1.60e-08\n",
      "Epoch 4, Step 701, Loss: 0.031285, VQ Loss: 0.003487, Diff Loss: 0.027798, learning rate: 1.60e-08\n",
      "Epoch 4, Step 1201, Loss: 0.031096, VQ Loss: 0.003411, Diff Loss: 0.027685, learning rate: 1.60e-08\n",
      "Epoch 4, Step 1701, Loss: 0.029136, VQ Loss: 0.003410, Diff Loss: 0.025726, learning rate: 1.60e-08\n",
      "Epoch 4, Step 2201, Loss: 0.033344, VQ Loss: 0.003475, Diff Loss: 0.029869, learning rate: 1.60e-08\n",
      "Epoch 4, Step 2701, Loss: 0.031356, VQ Loss: 0.003464, Diff Loss: 0.027892, learning rate: 1.60e-08\n",
      "Epoch 4, Step 3201, Loss: 0.030125, VQ Loss: 0.003430, Diff Loss: 0.026696, learning rate: 1.60e-08\n",
      "Epoch 4, Step 3701, Loss: 0.030681, VQ Loss: 0.003412, Diff Loss: 0.027269, learning rate: 1.60e-08\n",
      "Epoch 4, Step 4201, Loss: 0.031095, VQ Loss: 0.003468, Diff Loss: 0.027627, learning rate: 1.60e-08\n",
      "Epoch 4, Step 4701, Loss: 0.030014, VQ Loss: 0.003486, Diff Loss: 0.026527, learning rate: 1.60e-08\n",
      "Epoch 4, Step 5201, Loss: 0.029967, VQ Loss: 0.003447, Diff Loss: 0.026520, learning rate: 1.60e-08\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 121\u001b[0m\n\u001b[1;32m    119\u001b[0m         test_rmse \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(loss_test)\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(test_rmse):\n\u001b[0;32m--> 121\u001b[0m             test_rmse_all\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtest_rmse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    123\u001b[0m current_rmse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(test_rmse_all) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_rmse_all)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m moving_avg_test_rmse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "n_epochs = 4\n",
    "eval_step = 5000\n",
    "save_every = 10000\n",
    "patience = 8  # Number of evaluations to wait for improvement\n",
    "cooldown = 4  # Evaluations to wait after an improvement before counting non-improvements\n",
    "smooth_factor = 0.6  # Smoothing factor for moving average\n",
    "lambda_flow = 1e-3  # Weight for flow matching loss\n",
    "print_every = 500\n",
    "\n",
    "# Setup\n",
    "train_all = len(train)\n",
    "model_name = \"TFT_Flowmatching\"\n",
    "from collections import defaultdict\n",
    "loss_all = defaultdict(list)\n",
    "best_test_rmse = float('inf')\n",
    "early_stopping_counter = 0\n",
    "cooldown_counter = cooldown\n",
    "\n",
    "now = datetime.now()\n",
    "folder_name = now.strftime(\"%b%d_%H-%M-%S\")\n",
    "print(f\"Saving model at ../model/{model_name}/{folder_name}\")\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=min(len(train) * n_epochs, 100000), eta_min=1e-8)\n",
    "# Define scheduler: ReduceLROnPlateau\n",
    "\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',        # 'min' because we want to minimize loss\n",
    "    factor=0.2,        # Reduce LR by factor of 0.2 (i.e., lr / 5)\n",
    "    patience=3000,     # Number of steps with no significant improvement before reducing LR\n",
    "    threshold=5e-4,    # Minimum change in loss to qualify as \"significant\"\n",
    "    min_lr=1e-8,       # Minimum LR to stop at\n",
    "    verbose=True       # Prints a message when LR is reduced\n",
    ")\n",
    "\n",
    "os.makedirs(f'../model/{model_name}/{folder_name}', exist_ok=True)\n",
    "\n",
    "\n",
    "# Initialize moving average\n",
    "moving_avg_test_rmse = None\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for step, (X_batch, y_batch) in tqdm(enumerate(train), total=train_all):\n",
    "        X_batch = X_batch.float().to(device)\n",
    "        y_batch = y_batch.float().to(device)\n",
    "        \n",
    "        current_pos_input = X_batch[:, -1, :2].clone().unsqueeze(1).repeat(1, lookback, 1)\n",
    "        current_pos_output = X_batch[:, -1, :2].clone().unsqueeze(1).repeat(1, future_steps, 1)\n",
    "        X_batch[:, :, :2] = X_batch[:, :, :2] - current_pos_input\n",
    "        y_batch[:, :, :2] = y_batch[:, :, :2] - current_pos_output\n",
    "\n",
    "        # # only take 0, 2, 4, 6, 8, 10, 12, 14, 16, 18\n",
    "        # y_batch = y_batch[:, ::2, :2]\n",
    "        # X_batch = X_batch[:, ::2, :]\n",
    "\n",
    "        X_batch = F.pad(X_batch, (0, feature_dim - X_batch.shape[-1]))\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # y_pred, vq_loss, perplexity = model(X_batch, y_batch=y_batch)\n",
    "        # loss = loss_fn(y_pred[:, :future_steps, :2], y_batch[:, :future_steps, :2])\n",
    "        diff_loss, vq_loss = model(X_batch, y_batch[:, :future_steps, :2])\n",
    "\n",
    "\n",
    "        loss_all['diff_loss'].append(diff_loss.item())\n",
    "        loss_all['vq_loss'].append(vq_loss.item() * 10)\n",
    "        \n",
    "        loss_all['loss'].append(diff_loss.item() + vq_loss.item() * 10)\n",
    "        # add vq_loss\n",
    "        loss = diff_loss  + 10 * vq_loss\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss.item())\n",
    "\n",
    "        if (epoch * train_all + step + 1) % print_every == 0:\n",
    "            loss_item = sum(loss_all['loss'][-100:]) / 100\n",
    "            vq_loss_item = sum(loss_all['vq_loss'][-100:]) / 100\n",
    "            diff_loss_item = sum(loss_all['diff_loss'][-100:]) / 100\n",
    "            print(f\"Epoch {epoch+1}, Step {step+1}, Loss: {loss_item:.6f}, VQ Loss: {vq_loss_item:.6f}, Diff Loss: {diff_loss_item:.6f}, learning rate: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "        \n",
    "        # Save model\n",
    "        if (epoch * train_all + step + 1) % save_every == 0:\n",
    "            os.makedirs(f'../model/{model_name}/{folder_name}', exist_ok=True)\n",
    "            save_path = f\"../model/{model_name}/{folder_name}/model_{epoch * train_all + step + 1}.pt\"\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Model saved at {save_path}\")\n",
    "\n",
    "        # Validation and early stopping\n",
    "        if (epoch * train_all + step + 1) % eval_step == 0:\n",
    "            model.eval()\n",
    "            test_rmse_all = []\n",
    "            with torch.no_grad():\n",
    "                for X_test_batch, y_test_batch in test:\n",
    "                    X_test_batch = X_test_batch.float().to(device)\n",
    "                    y_test_batch = y_test_batch.float().to(device)\n",
    "                    \n",
    "                    current_pos_input = X_test_batch[:, -1, :2].clone().unsqueeze(1).repeat(1, lookback, 1)\n",
    "                    current_pos_output = X_test_batch[:, -1, :2].clone().unsqueeze(1).repeat(1, future_steps, 1)\n",
    "                    X_test_batch[:, :, :2] = X_test_batch[:, :, :2] - current_pos_input\n",
    "                    y_test_batch[:, :, :2] = y_test_batch[:, :, :2] - current_pos_output\n",
    "\n",
    "                    # # only take 0, 2, 4, 6, 8, 10, 12, 14, 16, 18\n",
    "                    # y_test_batch = y_test_batch[:, ::2, :2]\n",
    "                    # X_test_batch = X_test_batch[:, ::2, :]\n",
    "                    \n",
    "                    X_test_batch = F.pad(X_test_batch, (0, feature_dim - X_test_batch.shape[-1]))\n",
    "\n",
    "\n",
    "                    y_pred_test = model(X_test_batch, influence=True)\n",
    "                    loss_test = loss_fn(y_pred_test[:, :future_steps, :2], y_test_batch[:, :future_steps, :2])\n",
    "                    test_rmse = torch.sqrt(loss_test)\n",
    "                    if not torch.isnan(test_rmse):\n",
    "                        test_rmse_all.append(test_rmse.item())\n",
    "            \n",
    "            current_rmse = sum(test_rmse_all) / len(test_rmse_all)\n",
    "            if moving_avg_test_rmse is None:\n",
    "                moving_avg_test_rmse = current_rmse\n",
    "            else:\n",
    "                moving_avg_test_rmse = smooth_factor * current_rmse + (1 - smooth_factor) * moving_avg_test_rmse\n",
    "\n",
    "            print(f\"Steps {epoch * train_all + step + 1}: test RMSE {current_rmse:.4f}, moving average RMSE {moving_avg_test_rmse:.4f}, learning rate {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "            # Check if the moving average RMSE is better; if not, increment counter\n",
    "            if moving_avg_test_rmse < best_test_rmse:\n",
    "                best_test_rmse = moving_avg_test_rmse\n",
    "                early_stopping_counter = 0  # Reset counter\n",
    "                cooldown_counter = cooldown  # Reset cooldown\n",
    "                # Optionally save the best model\n",
    "                os.makedirs(f'../model/{model_name}/{folder_name}', exist_ok=True)\n",
    "                best_model_path = f\"../model/{model_name}/{folder_name}/best_model.pt\"\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "            else:\n",
    "                if cooldown_counter > 0:\n",
    "                    cooldown_counter -= 1\n",
    "                else:\n",
    "                    early_stopping_counter += 1\n",
    "\n",
    "            if early_stopping_counter >= patience:\n",
    "                print(f\"Stopping early at epoch {epoch+1}, step {step+1}\")\n",
    "                break\n",
    "\n",
    "            model.train()\n",
    "        \n",
    "    if early_stopping_counter >= patience:\n",
    "        break\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a8f21561b1e4d91a56f8548665a10b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/138 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 1.0846864563831384\n"
     ]
    }
   ],
   "source": [
    "validation_step = future_steps\n",
    "\n",
    "predictions = []\n",
    "truths = []\n",
    "\n",
    "max_test_batch = 200\n",
    "cur_test = 0\n",
    "\n",
    "test_loss_all = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    all_test = len(test)\n",
    "    test_rmse_all = []\n",
    "    for X_test_batch, y_test_batch in tqdm(test):\n",
    "        cur_test += 1\n",
    "        if cur_test > max_test_batch:\n",
    "            break\n",
    "\n",
    "        X_test_batch = X_test_batch.float().to(device)\n",
    "        y_test_batch = y_test_batch.float().to(device)\n",
    "        \n",
    "        current_pos_input = X_test_batch[:, -1, :2].clone().unsqueeze(1).repeat(1, lookback, 1)\n",
    "        current_pos_output = X_test_batch[:, -1, :2].clone().unsqueeze(1).repeat(1, future_steps, 1)\n",
    "        X_test_batch[:, :, :2] = X_test_batch[:, :, :2] - current_pos_input\n",
    "        y_test_batch[:, :, :2] = y_test_batch[:, :, :2] - current_pos_output\n",
    "\n",
    "        X_test_batch = F.pad(X_test_batch, (0, feature_dim - X_test_batch.shape[-1]))\n",
    "\n",
    "\n",
    "        # # only take 0, 2, 4, 6, 8, 10, 12, 14, 16, 18\n",
    "        # y_test_batch = y_test_batch[:, ::2, :2]\n",
    "        # X_test_batch = X_test_batch[:, ::2, :]\n",
    "        \n",
    "        y_preds = model(X_test_batch, influence=True, return_all=True)\n",
    "        # slect the one with minimum loss\n",
    "\n",
    "        min_loss = float('inf')\n",
    "        best_pred = None\n",
    "        for y_pred in y_preds:\n",
    "            loss_test = loss_fn(y_pred[:, :future_steps, :2], y_test_batch[:, :future_steps, :2])\n",
    "            test_rmse = torch.sqrt(loss_test)\n",
    "            if test_rmse < min_loss:\n",
    "                min_loss = test_rmse\n",
    "                best_pred = y_pred\n",
    "        \n",
    "        test_loss_all.append(min_loss.item())\n",
    "\n",
    "        predictions.append(y_pred[:, :validation_step, :2] + current_pos_output[:, :y_pred.shape[1], :2])\n",
    "        truths.append(y_test_batch[:, :validation_step, :2] + current_pos_output[:, :y_pred.shape[1], :2])\n",
    "\n",
    "\n",
    "print(f\"Test RMSE: {sum(test_loss_all) / len(test_loss_all)}\")       \n",
    "predictions = torch.cat(predictions, dim=0)\n",
    "truths = torch.cat(truths, dim=0)\n",
    "\n",
    "# reverse normalization\n",
    "normalize_dict = stats_dict\n",
    "\n",
    "for idx, key_ in enumerate([\"User_X\", \"User_Y\"]):\n",
    "    predictions[:, :, idx] = predictions[:, :, idx] * (normalize_dict['max'][key_] - normalize_dict['min'][key_]) + normalize_dict['min'][key_]\n",
    "    predictions[:, :, idx] = predictions[:, :, idx] * normalize_dict['std'][key_] + normalize_dict['mean'][key_]\n",
    "    truths[:, :, idx] = truths[:, :, idx] * (normalize_dict['max'][key_] - normalize_dict['min'][key_]) + normalize_dict['min'][key_]\n",
    "    truths[:, :, idx] = truths[:, :, idx] * normalize_dict['std'][key_] + normalize_dict['mean'][key_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assume predictions and truths are torch tensors of shape (batch_size, num_steps, num_dims)\n",
    "# # Also assume model_name and folder_name are defined strings for saving the plot.\n",
    "# criterion = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "# steps = []\n",
    "# ade_loss = []\n",
    "# fde_loss = []\n",
    "\n",
    "# # Loop over each prediction horizon (step)\n",
    "# for step in range(1, predictions.size(1) + 1):\n",
    "#     # Compute MSE loss for the first 'step' timesteps for all samples\n",
    "#     raw_loss = criterion(predictions[:, :step, :], truths[:, :step, :])\n",
    "#     # Sum loss over the coordinate dimension and take square root to get RMSE per timestep per sample\n",
    "#     raw_rmse = torch.sqrt(torch.sum(raw_loss, dim=-1))\n",
    "    \n",
    "#     # ADE: average RMSE over all time steps for each sample\n",
    "#     current_ade = raw_rmse.mean(dim=-1)\n",
    "#     # FDE: RMSE error at the final timestep for each sample\n",
    "#     current_fde = raw_rmse[:, -1]  # Alternatively, use: raw_rmse.max(dim=-1).values\n",
    "    \n",
    "#     ade_loss.append(current_ade)\n",
    "#     fde_loss.append(current_fde)\n",
    "#     steps.extend([step] * len(current_ade))\n",
    "\n",
    "# # Concatenate results across all steps and move to CPU numpy arrays\n",
    "# ade_loss = torch.cat(ade_loss).cpu().numpy()\n",
    "# fde_loss = torch.cat(fde_loss).cpu().numpy()\n",
    "\n",
    "# # Create DataFrames for ADE and FDE\n",
    "# df_ade = pd.DataFrame({'Step': steps, 'Loss': ade_loss, 'Metric': 'ADE'})\n",
    "# df_fde = pd.DataFrame({'Step': steps, 'Loss': fde_loss, 'Metric': 'FDE'})\n",
    "\n",
    "# # Combine both DataFrames\n",
    "# df = pd.concat([df_ade, df_fde], ignore_index=True)\n",
    "\n",
    "# # Convert step count to seconds and scale the RMSE error to meters\n",
    "# df['Second (s)'] = df['Step'] / 10   # For example, if 5 steps equal 1 second\n",
    "# df['RMSE Error (m)'] = df['Loss'] / 100  # Convert error to meters\n",
    "\n",
    "# # Plot the ADE and FDE curves using seaborn\n",
    "# sns.lineplot(data=df, x='Second (s)', y='RMSE Error (m)', hue='Metric')\n",
    "# plt.title(\"Trajectory Prediction Error: ADE vs FDE\")\n",
    "# plt.xlabel(\"Time (s)\")\n",
    "# plt.ylabel(\"RMSE Error (m)\")\n",
    "# # plt.savefig(f'../model/{model_name}/{folder_name}/res.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "steps = []\n",
    "loss = []\n",
    "max_loss = []\n",
    "for step in range(1, predictions.size(1) + 1):\n",
    "    raw_rmse_loss = criterion(predictions[:, :step, :], truths[:, :step, :])\n",
    "    raw_rmse_loss = torch.sqrt(torch.sum(raw_rmse_loss, dim=-1))\n",
    "    mean_rmse_loss = raw_rmse_loss.mean(dim=-1)\n",
    "    max_rmse_loss = raw_rmse_loss.max(dim=-1).values\n",
    "    loss.append(mean_rmse_loss)\n",
    "    max_loss.append(max_rmse_loss)\n",
    "    steps.extend([step] * len(mean_rmse_loss))\n",
    "    \n",
    "max_loss = torch.cat(max_loss).cpu().numpy()\n",
    "loss = torch.cat(loss).cpu().numpy()\n",
    "\n",
    "df = pd.DataFrame({'Second (s)': steps, 'loss': loss})\n",
    "df1 = pd.DataFrame({'Second (s)': steps, 'loss': max_loss})\n",
    "df['type'] = 'mean'\n",
    "df1['type'] = 'max'\n",
    "df = pd.concat([df, df1])\n",
    "\n",
    "\n",
    "df['RMSE Error (m)'] = df['loss'] / 100 # to meters\n",
    "df['Second (s)'] = df['Second (s)'] / 10 # to seconds\n",
    "sns.lineplot(data = df, x='Second (s)', y='RMSE Error (m)', hue='type',) #  errorbar=('sd', 1),\n",
    "plt.savefig(f'../model/{model_name}/{folder_name}/res.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize\n",
    "\n",
    "time_stamp = 25599\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "pred = predictions[time_stamp, :, :].cpu().numpy()\n",
    "truth = truths[time_stamp, :, :].cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# equal axis\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "ax.plot(pred[:, 0], pred[:, 1], color='blue', label='Predicted Trajectory')\n",
    "ax.plot(truth[:, 0], truth[:, 1], color='red', label='Ground Truth Trajectory')\n",
    "ax.set_xlabel('X Coordinate')\n",
    "ax.set_ylabel('Y Coordinate')\n",
    "ax.set_title('Predicted vs Ground Truth Trajectory')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>type</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Second (s)</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>0.171461</td>\n",
       "      <td>0.171461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>0.223241</td>\n",
       "      <td>0.178003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>0.270057</td>\n",
       "      <td>0.189243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>0.313821</td>\n",
       "      <td>0.201469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.353750</td>\n",
       "      <td>0.213341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6</th>\n",
       "      <td>0.397799</td>\n",
       "      <td>0.227123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7</th>\n",
       "      <td>0.439280</td>\n",
       "      <td>0.240680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.482008</td>\n",
       "      <td>0.254722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.9</th>\n",
       "      <td>0.526203</td>\n",
       "      <td>0.268891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>0.569242</td>\n",
       "      <td>0.282290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.1</th>\n",
       "      <td>0.614529</td>\n",
       "      <td>0.296174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.2</th>\n",
       "      <td>0.661918</td>\n",
       "      <td>0.310520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.3</th>\n",
       "      <td>0.709374</td>\n",
       "      <td>0.324744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.4</th>\n",
       "      <td>0.756445</td>\n",
       "      <td>0.338989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.5</th>\n",
       "      <td>0.805325</td>\n",
       "      <td>0.353210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.6</th>\n",
       "      <td>0.853202</td>\n",
       "      <td>0.367111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.7</th>\n",
       "      <td>0.902458</td>\n",
       "      <td>0.381139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.8</th>\n",
       "      <td>0.953483</td>\n",
       "      <td>0.395237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.9</th>\n",
       "      <td>1.004259</td>\n",
       "      <td>0.409429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1.057330</td>\n",
       "      <td>0.423669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.1</th>\n",
       "      <td>1.109570</td>\n",
       "      <td>0.437915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.2</th>\n",
       "      <td>1.165037</td>\n",
       "      <td>0.452112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.3</th>\n",
       "      <td>1.218427</td>\n",
       "      <td>0.466211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.4</th>\n",
       "      <td>1.274955</td>\n",
       "      <td>0.480769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5</th>\n",
       "      <td>1.333376</td>\n",
       "      <td>0.495238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.6</th>\n",
       "      <td>1.391937</td>\n",
       "      <td>0.509853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.7</th>\n",
       "      <td>1.450482</td>\n",
       "      <td>0.524455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.8</th>\n",
       "      <td>1.506862</td>\n",
       "      <td>0.538694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.9</th>\n",
       "      <td>1.565806</td>\n",
       "      <td>0.553237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>1.627988</td>\n",
       "      <td>0.568138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.1</th>\n",
       "      <td>1.684753</td>\n",
       "      <td>0.582717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.2</th>\n",
       "      <td>1.742949</td>\n",
       "      <td>0.597408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.3</th>\n",
       "      <td>1.805792</td>\n",
       "      <td>0.612393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.4</th>\n",
       "      <td>1.866814</td>\n",
       "      <td>0.627173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.5</th>\n",
       "      <td>1.929212</td>\n",
       "      <td>0.642130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.6</th>\n",
       "      <td>1.991468</td>\n",
       "      <td>0.657165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.7</th>\n",
       "      <td>2.052979</td>\n",
       "      <td>0.672087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.8</th>\n",
       "      <td>2.116125</td>\n",
       "      <td>0.687298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.9</th>\n",
       "      <td>2.179750</td>\n",
       "      <td>0.702486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>2.246368</td>\n",
       "      <td>0.717984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "type             max      mean\n",
       "Second (s)                    \n",
       "0.1         0.171461  0.171461\n",
       "0.2         0.223241  0.178003\n",
       "0.3         0.270057  0.189243\n",
       "0.4         0.313821  0.201469\n",
       "0.5         0.353750  0.213341\n",
       "0.6         0.397799  0.227123\n",
       "0.7         0.439280  0.240680\n",
       "0.8         0.482008  0.254722\n",
       "0.9         0.526203  0.268891\n",
       "1.0         0.569242  0.282290\n",
       "1.1         0.614529  0.296174\n",
       "1.2         0.661918  0.310520\n",
       "1.3         0.709374  0.324744\n",
       "1.4         0.756445  0.338989\n",
       "1.5         0.805325  0.353210\n",
       "1.6         0.853202  0.367111\n",
       "1.7         0.902458  0.381139\n",
       "1.8         0.953483  0.395237\n",
       "1.9         1.004259  0.409429\n",
       "2.0         1.057330  0.423669\n",
       "2.1         1.109570  0.437915\n",
       "2.2         1.165037  0.452112\n",
       "2.3         1.218427  0.466211\n",
       "2.4         1.274955  0.480769\n",
       "2.5         1.333376  0.495238\n",
       "2.6         1.391937  0.509853\n",
       "2.7         1.450482  0.524455\n",
       "2.8         1.506862  0.538694\n",
       "2.9         1.565806  0.553237\n",
       "3.0         1.627988  0.568138\n",
       "3.1         1.684753  0.582717\n",
       "3.2         1.742949  0.597408\n",
       "3.3         1.805792  0.612393\n",
       "3.4         1.866814  0.627173\n",
       "3.5         1.929212  0.642130\n",
       "3.6         1.991468  0.657165\n",
       "3.7         2.052979  0.672087\n",
       "3.8         2.116125  0.687298\n",
       "3.9         2.179750  0.702486\n",
       "4.0         2.246368  0.717984"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results = df.groupby(by=['Second (s)', 'type']).mean().unstack()['RMSE Error (m)']\n",
    "exp_results.to_csv(f'../model/{model_name}/{folder_name}/result.csv')\n",
    "exp_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export JIT Model\n",
    "\n",
    "Integrate partial of data processing into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EXPORT_MODEL = True\n",
    "\n",
    "# # # model.load_state_dict(torch.load(\"/home/shaoze/Documents/Boeing/Boeing-Trajectory-Prediction/model/Jul09_20-37-37/model_40000.pt\"))\n",
    "# # if EXPORT_MODEL:\n",
    "# #     model.eval()\n",
    "# #     model.to('cpu')\n",
    "# #     script_module = torch.jit.script(model)\n",
    "# #     os.makedirs(f'../model/exported/', exist_ok=True)\n",
    "# #     script_module.save(\"../exported/model_tft_vqvae_cpu.pt\")\n",
    "\n",
    "# stats = {}\n",
    "# '''\n",
    "# mean: tensor[]\n",
    "# '''\n",
    "\n",
    "# for keys, values in stats_dict.items():\n",
    "#     stats[keys] = torch.tensor(values.to_list()).view(1,1,-1)\n",
    "    \n",
    "# class TFT_EXP(nn.Module):\n",
    "#     def __init__(self, model:EnhancedTFT, stats:dict):\n",
    "#         super(TFT_EXP, self).__init__()\n",
    "#         self.stats = stats\n",
    "#         self.register_buffer('mean', self.stats['mean'])\n",
    "#         self.register_buffer('std', self.stats['std'])\n",
    "#         self.register_buffer('min', self.stats['min'])\n",
    "#         self.register_buffer('max', self.stats['max'])\n",
    "#         self.TFT = model\n",
    "#         self.num_steps = model.num_steps\n",
    "#         self.num_outputs = model.num_outputs # =2\n",
    "\n",
    "#     def forward(self, x, mask: Optional[torch.Tensor]=None):\n",
    "#         single = False\n",
    "#         if len(x.shape) == 2:\n",
    "#             x = x.unsqueeze(0)\n",
    "#             single = True\n",
    "        \n",
    "#         # normalize\n",
    "#         x = (x - self.mean) / self.std\n",
    "#         x = (x - self.min) / (self.max - self.min)\n",
    "#         # residual\n",
    "#         current_pos_input = x[:, -1, :2].clone().unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "#         current_pos_output = x[:, -1, :2].clone().unsqueeze(1).repeat(1, self.num_steps, 1)\n",
    "#         x[:, :, :2] = x[:, :, :2] - current_pos_input\n",
    "        \n",
    "#         # pass through TFT\n",
    "#         outputs, vq_loss, perplexity = self.TFT(x, mask)\n",
    "#         outputs = outputs.detach()\n",
    "        \n",
    "#         # de-residual\n",
    "#         outputs[:, :, :2] = outputs[:, :, :2] + current_pos_output\n",
    "        \n",
    "#         # denormalize\n",
    "#         outputs = outputs * (self.max[:,:,:self.num_outputs] - self.min[:,:,:self.num_outputs]) + self.min[:,:,:self.num_outputs]\n",
    "#         outputs = outputs * self.std[:,:,:self.num_outputs] + self.mean[:,:,:self.num_outputs]\n",
    "        \n",
    "#         if single:\n",
    "#             outputs = outputs.squeeze(0)\n",
    "#         return outputs\n",
    "\n",
    "# tft_exp = TFT_EXP(model, stats)\n",
    "# tft_exp.to('cpu')\n",
    "# tft_exp.eval()\n",
    "# # script_module = torch.jit.script(tft_exp)\n",
    "# # os.makedirs(f'../model/exported/', exist_ok=True)\n",
    "# # script_module.save(\"../exported/model_tft_vqvae_cpu_preproc.pt\")\n",
    "\n",
    "# # export to onnx\n",
    "\n",
    "# dummy_input = torch.randn(1, lookback, feature_dim)\n",
    "# print(f\"Input shape: {dummy_input.shape}\")\n",
    "\n",
    "# # Export the wrapped model to ONNX format\n",
    "# torch.onnx.export(\n",
    "#     tft_exp,                   # Wrapped model to export\n",
    "#     dummy_input,                     # Model input\n",
    "#     \"../exported/tft_1111.onnx\",              # Output file name\n",
    "#     export_params=True,              # Store the trained parameter weights inside the model file\n",
    "#     opset_version=13,                # Set the ONNX opset version (adjust as needed)\n",
    "#     do_constant_folding=True,        # Whether to execute constant folding for optimization\n",
    "#     input_names=['input'],           # The model's input names\n",
    "#     output_names=['output'],         # The model's output names\n",
    "#     # dynamic_axes={\n",
    "#     #     'input': {0: 'batch_size'},  # Dynamic batch_size and sequence_length\n",
    "#     #     'output': {0: 'batch_size'}  # Dynamic batch_size for the output\n",
    "#     # }\n",
    "# )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import onnxruntime as ort\n",
    "# import numpy as np\n",
    "\n",
    "# # Path to your ONNX model\n",
    "# model_path = \"../exported/tft_1111.onnx\"\n",
    "\n",
    "# # Create an inference session\n",
    "# session = ort.InferenceSession(model_path)\n",
    "\n",
    "# # Get the name of the input node\n",
    "# input_name = session.get_inputs()[0].name\n",
    "\n",
    "# for file in os.listdir(dir):\n",
    "#     if file.endswith('.pkl'):\n",
    "#         df = process_data(dir+file)\n",
    "#     break\n",
    "\n",
    "# df = df[['User_X', 'User_Y', 'AGV_distance_X', 'AGV_distance_Y', 'AGV_speed_X',\n",
    "#        'AGV_speed_Y', 'AGV_speed', 'User_speed_X', 'User_speed_Y',\n",
    "#        'User_speed', 'User_velocity_X', 'User_velocity_Y', 'Wait_time',\n",
    "#        'intent_to_cross', 'Gazing_station', 'possible_interaction',\n",
    "#        'facing_along_sidewalk', 'facing_to_road', 'On_sidewalks', 'On_road',\n",
    "#        'closest_station', 'distance_to_closest_station',\n",
    "#        'distance_to_closest_station_X', 'distance_to_closest_station_Y',\n",
    "#        'looking_at_AGV', 'GazeDirection_X', 'GazeDirection_Y',\n",
    "#        'GazeDirection_Z', 'AGV_X', 'AGV_Y',\n",
    "#        'looking_at_closest_station']]\n",
    "\n",
    "# start_idx = 100\n",
    "# input = df.iloc[200:200+lookback].astype(np.float32).values\n",
    "\n",
    "# # add batch\n",
    "# input = input[np.newaxis, :, :]\n",
    "# # Run the model\n",
    "# output = session.run(None, {input_name: input.astype(np.float32)})[0]\n",
    "\n",
    "# output\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save data (for interactive visualization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.jit.load(\"../exported/model_tft_vqvae_cpu.pt\")\n",
    "\n",
    "# test_ds = MyDataset(lookback=lookback)\n",
    "# all_ds = ds.dataset\n",
    "# test_ds.dataset = all_ds[len(all_ds)//10 :] # load the last 10% of the data\n",
    "# X_list, y_list = test_ds.generate_data(return_list=True, future_steps=future_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "# normalize_dict = stats_dict\n",
    "# pred_data = []\n",
    "# truth_data = []\n",
    "# input_data = []\n",
    "# model.eval()\n",
    "# device = 'cpu'\n",
    "# for i, (X, y) in enumerate(zip(X_list, y_list)):\n",
    "#     current_pos_input = X[:, -1, :2].clone().unsqueeze(1).repeat(1, lookback, 1)\n",
    "#     current_pos_output = X[:, -1, :2].clone().unsqueeze(1).repeat(1, future_steps, 1).to(device)\n",
    "#     X[:, :, :2] = X[:, :, :2] - current_pos_input\n",
    "\n",
    "#     predictions = model(X.float().to(device))[0][:, :future_steps, :2]\n",
    "#     predictions = predictions + current_pos_output\n",
    "#     predictions = predictions.to('cpu')\n",
    "    \n",
    "#     truths = y[:, :future_steps, :2]\n",
    "#     X[:, :, :2] = X[:, :, :2] + current_pos_input\n",
    "#     model_input = X.float().to(device)[:, :lookback, :2]\n",
    "#     trajectory_id = i\n",
    "    \n",
    "#     # reverse normalization\n",
    "#     for idx, key_ in enumerate([\"User_X\", \"User_Y\"]):\n",
    "#         predictions[:, :, idx] = predictions[:, :, idx] * (normalize_dict['max'][key_] - normalize_dict['min'][key_]) + normalize_dict['min'][key_]\n",
    "#         predictions[:, :, idx] = predictions[:, :, idx] * normalize_dict['std'][key_] + normalize_dict['mean'][key_]\n",
    "#         truths[:, :, idx] = truths[:, :, idx] * (normalize_dict['max'][key_] - normalize_dict['min'][key_]) + normalize_dict['min'][key_]\n",
    "#         truths[:, :, idx] = truths[:, :, idx] * normalize_dict['std'][key_] + normalize_dict['mean'][key_]\n",
    "#         model_input[:, :, idx] = model_input[:, :, idx] * (normalize_dict['max'][key_] - normalize_dict['min'][key_]) + normalize_dict['min'][key_]\n",
    "#         model_input[:, :, idx] = model_input[:, :, idx] * normalize_dict['std'][key_] + normalize_dict['mean'][key_]\n",
    "    \n",
    "#     for group_id in range(predictions.shape[0]):\n",
    "#         for time_step in range(predictions.shape[1]):\n",
    "#             pred_x, pred_y = predictions[group_id, time_step]\n",
    "#             pred_data.append([trajectory_id, group_id, time_step, pred_x.item(), pred_y.item()])\n",
    "\n",
    "#             truth_x, truth_y = truths[group_id, time_step]\n",
    "#             truth_data.append([trajectory_id, group_id, time_step, truth_x.item(), truth_y.item()])\n",
    "        \n",
    "#         for time_step in range(lookback):\n",
    "#             input_x, input_y = model_input[group_id, time_step]\n",
    "#             input_data.append([trajectory_id, group_id, time_step, input_x.item(), input_y.item()])\n",
    "            \n",
    "\n",
    "# pred_df = pd.DataFrame(pred_data, columns=['trajectory_id', 'Group_ID', 'Time_Step', 'X', 'Y'])\n",
    "# truth_df = pd.DataFrame(truth_data, columns=['trajectory_id', 'Group_ID', 'Time_Step', 'X', 'Y'])\n",
    "# input_df = pd.DataFrame(input_data, columns=['trajectory_id', 'Group_ID', 'Time_Step', 'X', 'Y'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_to_remove = [\n",
    "#     \"../data/pred_tra_all.pkl\",\n",
    "#     \"../data/truth_tra_all.pkl\", \n",
    "#     \"../data/input_tra_all.pkl\"\n",
    "# ]\n",
    "\n",
    "# for file_path in files_to_remove:\n",
    "#     if os.path.exists(file_path):\n",
    "#         os.remove(file_path)\n",
    "\n",
    "# truth_df.to_pickle(\"../data/truth_tra_all.pkl\")\n",
    "# pred_df.to_pickle(\"../data/pred_tra_all.pkl\")\n",
    "# input_df.to_pickle(\"../data/input_tra_all.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
